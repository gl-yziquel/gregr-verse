===============================================================================
= current priorities
===============================================================================
high level syntax
  - tagged values and high level step, as described for syntax-1 below
  - include quoting/syntax-abstraction both before and after eval is defined
  - basic data type definitions
  - basic modules

concrete syntaxes
  syntax-0 = concrete syntax for raw terms that elides action/value wrappers, with pretty substitutions
  syntax-0 step = single raw term step
  syntax-1 = concrete syntax for higher-level terms including only the following:
    - variables, syntax-1-tagged values
    - substitutions of only syntax-1-tagged values over a syntax-1 term
    - applications involving only other syntax-1 terms
  syntax-1 step = one unconditional syntax-0 step followed by as many syntax-0 steps as necessary to eliminate all non-syntax-1 terms
    - this may not always be possible: may hit an irreducible non-syntax-1 term
      - when starting with a syntax-1 term, this non-syntax-1 term should always end up being a value, thus terminating
        - unless there is a mistake in the design of syntax-1 primitives

program graph editor:
  look a few sections below
  implementation notes
    for AST annotations, describe them via paths descriptions rather than directly injecting decoration state into the AST itself
      - what I mean by path:
          annotate the term found by starting from the root and going left, right, left, left... etc.
      - allows diversity of presentations for the same AST
      - better for concurrency
      - easier to prove AST transformations correct
    factoring out sub-expressions:
      - push new substitution(s) upwards, in parallel
        - reverse application of explicit substitutions
      - eventually reify as a lambda application?
        - may only be necessary to cross upwards beyond other substitutions
          - that is, you form a lambda, let the upper subsitution cross you on its way down, then you keep moving up
      - probably need a beta reduction rule that allows substitution of an argument 'produce' term for a linear parameter in strict position
    syntactic issues
      - processing programs as data
        - keep in mind that programs are *not* text strings, though names may be
        - "concrete" syntax is made of lisp-like s-exprs
          - this form is produced by quoting of the program "text" and is what is passed as arguments to macros/fexprs
        - abstract syntax is made up of terms
          - this form is produced from concrete syntax and is what is actually manipulated by program transformations (such as eval)
          - this would be the form indicated by staging annotations (think MetaOCaml)
      - pretty presentation of terms involves
        - smart operator handling: moving from prefix form to other arbitrary forms
        - eliminating syntactic noise
        - effective spatial layout
        - highlighting and other decorations
    database representation
      - somewhat inspired by version-control, file systems, and the internet
      - each data object is immutable, with a UID (not necessarily a GUID)
      - hierarchical names as mutable references to data objects, somewhat like git branches
        - updates synchronized via compare-and-swap to guarantee update's intent
      - where do DB-based applications (text, gui, browser editors) store their state?
      - the hierarchical namespace is just a protocol that can be implemented in various ways
        - local persistence via disk
        - a remote machine's persistence accessed over the network
        - data services provided by local or remote applications
          - such as a term/program DB likely to be part of this program graph editor design...

===============================================================================
= effects by reflection
===============================================================================

Define effects as a consequence of a new term 'produce'.
  Previously, terms would only terminate when stuck, or after converging to a
  value.  The 'produce' term is a way of aborting a computation earlier, upon
  fully evaluating its argument.  The argument may then be interpreted by the
  meta-program (the program that is interpreting the program that has just
  halted on (produce v)) as an invocation of an external capability.  If this
  is intended, the invocation result should then be substituted for the
  (produce v) term, and the program resumed from that point.

  Depending on the meta-program capabilities and 'produce' communication
  protocol, this may enable effects such as i/o, mutable state,
  delimited control, concurrency... without complicating the fundamental
  theory.  Side-effectful extensions were originally planned to be implemented
  in entirely separate languages.  Now, effect handling becomes an orthogonal
  concern of certain host/meta-programs.

  It may seem as if this is punting on the hard work, but this approach is
  really just relocating it to a more appropriate context.  Rather than dealing
  with difficult theory for all programs, complications only need to be dealt
  with for the specific interpretations of 'produce' for which they are
  relevant.  A bonus of this approach is that we will also able to experiment
  with several effect system variations without having to reinvent the
  fundamental language each time.

- produce for i/o interaction and other effects
  - do we need unforgeable constants to securely represent capabilities when
    allocating/new-ref-ing?
    - or is it enough to check for absolute safety by verifying
      convergence/productivity of ref-allocating computation for all possible
      ref encodings that could be handed back to it?
      - attempts to cheat should always be foiled by this measure, since such
        programs have to guess a particular ref encoding, and shouldn't be able
        to guarantee they choose the right one every time

===============================================================================
= more interaction ideas
===============================================================================

- module-building syntax/mode/file-processing

- multi-hole/context editing and combining
  - free-hand inputting of new terms
    - 'transform': describe a function to be applied to current term
  - similar factoring primitives to those described in:
    http://worrydream.com/LearnableProgramming/

===============================================================================
= program graph editor
===============================================================================

manipulation:
  view and traverse space
    - maneuver to different term positions
    - visualize all or parts of terms
    - some form of structural diff and merge
  view and traverse time
    - something like sub-term evaluation tree thumbnail diagrams?
    - undo/redo transformations, revisit past evaluation steps
      - separate indication of the following transformations
        * deterministic steps (all contiguous steps part of the same thumbnail group)
        * non-deterministic steps (group by sub-term and contiguity of steps)
        * rewrites
        * complete replacements
    - view program transitions at flexible granularities
      - example: it should be possible to view program states corresponding to higher-level events, such as user interactions, video buffer refreshes, network events, etc.
      - this seems related to the programmable editing section
  manipulate terms
    - meaning-preserving transformations
      * evaluation steps
      * rewrites
      * conflict detection when historical dependencies change
        example:
          You reduce a term that depends on some binding.  Later, you change the binding.
          Warn if the former reduction would no longer produce the same result, as this may not be the programmer's intention.
    - introduce new terms
      - intelligent defaults for sub-terms
    - copy and replace terms
    - combine terms
      - example: build a compound term from existing sub-terms
    - lift out sub-expressions via functional or substitutional abstraction
    - abstract constants by replacing them with variable expressions with the intended meaning
      - example: focusing on the value of 'y'
        before
          x = 4; y = 20
        after
          x = 4; y = x * 5

navigation, visualization, organization:
  accessibility
    - configurable color palettes
    - optional multi-phased motions for increased precision
  prominently indicate the possible actions a programmer may take at any time
    - like a catalog of building blocks, list the primitive term/value components that may be instantiated
    - when choosing to perform an operation on existing components, make it apparent what components may be acted on
    - affordances
      - fixed arity terms show holes where subterms may be placed
      - n-ary terms allow insertion at any position, moving neighboring subterms out of the way
  visualize variable bindings
    - show the values all variables are bound to
    - show variables that are unbound
    - optionally show binding edges, connecting variable instances to their binder
  allow different versions of the same program to exist and be edited concurrently
    - it should be possible to move between these versions
    - different versions could be being edited simultaneously by multiple programmers
      - it should be possible to see what other programmers are currently doing or have done
  term organization and focusing
    - provide multiple workspaces
      - hierarchical or otherwise
      - share or move data/terms around between them
      - combine them
      - split them
      - possibly share with other programmers
    - allow several potentially-unrelated free-floating terms in a workspace
      - allow these to be grouped and moved around in groups
    - focus on subterms in-situ
    - pull out subterms into their own free-floating form that refers back to their position in the parent expression
    - hide or discard terms
      - completely hide to reduce clutter
      - make large or complicated terms less distracting by replacing them with a representative symbol
    - name and catalog terms and groups, free-floating or in-situ, for later search, retrieval and reference
    - identify multiple instances of the same term (copied or coincidentally identical)
  marks for jumping around more quickly
    - named
    - unnamed, stack-like, referenced by offset from head or tail
    - groupable, push/pop-able or saved/loaded by name
    - preserved under transformation where possible
  hierarchical naming and retrieval
    - hierarchy can describe categorizations, parent/child relation between terms, or other resource organizations
      - aside from terms, this should probably include the other editor elements, such as:
        - views themselves
          - views could be pointed either at terms, or at other views themselves to indicate the desire for some kind of soft-link-style sharing
        - marks
        - version history/pointers/logs
        - "compiled" programs
        - arbitrary non-term data
        - processes
    - sort of like urls; not really like a directory structure (parent and parent/child can both be terms/resources)
    - provides an intuitive organization, but doesn't reflect the exact data representation
      - for instance, significant amounts of associations and meta-data may be stored alongside the actual resources
        - potentially anything could be stored alongside
        - all without having to reify your meta organization in the hierarchy itself, as you might in a file system
    - support soft and hard linking
    - hierarchies should be easily duplicable/relocatable via grafting
    - should be publishable; should be able to refer to imported publications by something analogous to a hostname
      - imported publications should be mountable/graftable within your own local hierarchy
        - either as copies, or as live links to the remote source

programmable editing:
  example features
    - custom macros for automating common transformations
    - automation via pattern-matching
      - example: find all terms of form X and perform some transformation on them
  important properties to make these features possible
    - ability to recognize structures of interest
    - ability to compose transformations
    - ability to script other non-transformational editor behaviors
      - focusing
      - mapping controls/keys
      - remembering editor states
      - other forms of presentation/organization/book-keeping...

seamless meta-interpreter integration:
  recognize/visualize meaning of quoted terms
    - programs at meta-level N are treated as data by an interpreter at meta-level N+1
    - don't get stuck seeing these terms only as data
    - be able to manipulate these quotations as if they were the programs they represent
  prominently display meta-variables corresponding to the meta-interpreter's state with respect to terms
    - example: a meta-variable that reflects the state of a mutable store before and after some effectful term
  given an interpretation of 'produce' terms, can we automatically build useful visualizations that correspond to the interpreter's description of:
    - implicit or mutable state
    - control inversion
    - non-deterministic concurrency
    - etc?

===============================================================================
= high-level encoding
===============================================================================

- how to track function parameter names unintrusively?
  - maybe it's ok to annotate functions; interference with equalities probably
    won't occur in any meaningful way
- modules and linking
  - renaming imports/exports
  - narrowing exports
    - support both white and black listing of exported names
  - symmetric link/mix/combine
    - asymmetric linking of N + M can be implemented via: N + (M - N)
  - how to resolve mutually-recursive deps?
    - internally represent both cyclic and acyclic (over SCCs) dependency
      graphs?
  - should procedures also be packaged up with meta-data, like modules?
    - yes, at least include a basic type tag
    - juxtaposition as sugar for applying these packaged procs, rather than for
      applying low-level lambdas
      - for low-level application, introduce a 'lam-apply' syntax
  - parsing, syntax transformers and metacircular eval
    - partial fexprs
    - syntax default (#%app)
    - setting the default
    - whitelist or blacklist names
  - how to interact with the pre-desugared syntax directly?
    - syntax-0 is basically in direct correspondence with underlying terms
    - higher level syntaxes will be more challenging

===============================================================================
= logic issues
===============================================================================

- proof constructors
  - forall-term, forall-value (and somehow these may be
    mutually-recursive/inductive?
    - think: forall-term T. (P T) -> (value (lam T)))
  - forall-X and -> create new scopes, binding a meta-var or proof-reference
    that may be used within the internal proof
  - how to represent or? maybe full-blown sequents are valid
    - classical logic for describing program transformations may not prevent
      program terms themselves from encoding constructive logic

- non-termination theory
  - can function call termination analysis be accomplished without more special
    machinery than an induction rule over terms/values?

- reductio ad absurdum via hypothesis invalidation

===============================================================================
= reflection issues
===============================================================================

- how to load quoted code into meta environment?
  - consider the L+1 (meta) environment to be fixed
  - the L environment is being managed by L+1, and can have new code
    incorporated into it at that level already
  - should this be done by re-seating the entire process?
  - is there some explainable way to do it iteratively?

===============================================================================
= pretty old effect system ideas
===============================================================================

- analysis of concurrency with shared memory
  - track set of escaped minimal embeddings of mutable cells to prove
    properties in the presence of concurrency
  - example:
    - thread T reads from cell M
      - assume set of escaped minimal M embeddings contains only E[M]
    - what can be written to cell M, forall S. S[E[M]] ? these are the only
      things that need to be considered readable in thread T
      - show that under the assumption of thread T making no progress (after
        all, it's waiting to read and may be holding a "lock" that affects
        other threads' behaviors with respect to M), S[E[M]] either:
        - embeds (write M x) for a finite set of possible x; these are the only
          x's that thread T may end up reading in
        - does not embed (write M x); special case of the above
        - diverges before any (write M x)
      - without such proof, must assume: forall v. (write M v); this means
        thread T could read anything in

- shift/reset
  - when encountered, reset marks the log, and pushes its one-hole-context
  - when providing a value to the reset ohc, the result should be skeptical of
    any obligations incurred since the log marking, as these obligations could
    hide shifts
    - if obligations can't be analyzed to tell if they contain shift, reset's
      result should be an obligation that resumes this investigation when
      evaluated
  - btw, shift should not capture return-context

===============================================================================
= old ideas (if I bring these back, it will be in a different context)
===============================================================================

; note: propositions really describe properties of full computational states,
;       not just thunks: catalog is implicit in the following
; TODO: [how] should we represent quantifiers?
; TODO: can negation instead be nicely generalized as 'False' implication?
;       then generalize [con/di]vergence using negation, ===>, and quantifiers

prop = thunk ==> thunk  ; step relation (==>* for reflexive transitive closure)
     | converges thunk atom  ; thunk terminates with atom as result
     | diverges thunk  ; thunk fails to terminate or produces an error
     | thunk === thunk  ; judgmentally equal thunks (commutative)
     | atom == atom  ; judgmentally equal (commutative)
     | atom != atom  ; judgementally unequal (commutative)
     | type value-type atom  ; type constraint on atom

thunk = (term, env)

atom = unit | bit-0 | bit-1 | indirect UID | promise UID

value-type = Unit | Bit | Pair | Lam


derived props such as:
  thunk0 ==>* thunk1, where
    thunk0 === thunk1 OR
    thunk0 ==> thunk1 OR
    thunk0 ==>* thunkX AND thunkX ==>* thunk1
  thunk ===> atom, equivalent to something like:
    thunk ==>* ((val-a atom ; halt), ENV)

contradictions produced by
  converges and diverges props for the same thunk
  == and != for the same pair of atoms
  multiple type constraints for the same atom

implications in the prop framework defined as meta-level lambdas
props are then data processed and produced by meta-level programs


new machinery
  catalogs now include propositions, describing hypotheses and/or inferences
    hypotheses are introduced at the meta-level by implications
  prop contradiction eliminates the current state from set of possibilities
  separate forcing/updating of delayed thunks from parent computation state
    you evaluate the thunk against the halt continuation
      but with the parent catalog
  lambda skolemization introduces new catalog frame: parent frame is read-only
  case splitting duplicates catalog frame for each new state case
  case splitting allows conjectures of the following forms:
    value-type cases
      the type of a promise is speculated before type-dependent evaluation
        (in the case of pair-access for bits/pairs or application for lams)
      each value-type conjecture starts a new hypothetical world/state
      as far as i can see, this will finally result in three situations:
        all cases converge, type splitting was unnecessary
        all cases diverge, type splitting did not save us
        only one case converges, type splitting made a difference
      no special term rebuilding necessary for this type of split
    bit-0 vs. bit-1
      this corresponds to exploring conditional branches
      each hypothetical world is rebuilt as a branch of a new conditional
    converge vs. diverge
      this corresponds to obligations for evaluating potentially-unused thunks
      if convergence cannot be proven for a thunk, a program is obligated
        to evaluate it even if its result is never used
      this obligation is reflected during term rebuilding
      proving divergence for a program thunk implies full program divergence
  observations about evolving converge/diverge hypotheses during evaluation
    corresponds to generating an obligation and promise
    as the thunk unwinds, the potentially-diverging part contracts
    contraction results in a new case split
      new diverging case obviates the original case split entirely
      new converging case contradicts original diverging case, eliminating it
      so, only the new child split/obligation remains
  for term rebuilding
    may need to keep a mapping of representative thunks for promises:
      promise UID -> (thunk OR skolem)
      the thunk introducing a promise is its representative

===============================================================================
= older ideas
===============================================================================

 eager and lazy CBV operational semantics
   CBV describes observable semantics while lazy/eager describes operational strategy
 eager CBV
   constructors with args need to save one-hole contexts
   all terms in focus are evaluated to completion with result values being catalogued
 lazy CBV
   constructors with args do not save any one-hole contexts
     they can punt on evaluating their args
     punted terms are paired with current environment and catalogued as eval obligations
   rewinding to evaluate an obligation
     pop catalog entries until desired key is found
     save popped entries (in reverse) with old context
     begin evaluating a new state with the remaining catalog
       (state clg-oblig-term (return-context (old-cont old-env reversed-entries)) clg-oblig-env)
     when returning to former context, re-push its entries onto the new catalog
   the catalog as described is actually a special case of a more general 'effect log'
     entry types can be added for memory allocation and writes
 given a catalog partitioning at any event boundary: members of older part do not depend on members of newer
   minimize duplication when splitting worlds around a hypothetical equality
     assumption boundary must be made before any key that would depend on it
       given key D being guessed
         assumption entry must be made before first E depending on D's value
         there may be entries between D and its assumption entry
           this would be because they depend on D's effects, but don't depend on D's value
       if made earlier than existing assumption, must split the splits
         this will happen with out-of-order case analysis on opaque values:
           first, case-analysis occurs on some D that happens to depend on C
           later, in one branch of (case D), retroactive case-analysis occurs on C
           the case-analysis on C must be pulled above that of D
             C's assumption entry must be made earlier than D's entry
             new split muts be made earlier than existing split, duplicating that split
             some waste produced (hopefully only temporarily) for the branch of D not analyzing C
       older keys definitely don't depend on assumption and make up the old region
       newer keys that happen to also not depend on assumption may be moved across it into the old region
         moving across also requires no effect dependencies
       old region ends up shared by both hypothetical worlds
   cleaning up after a transformation attempt on a subterm
     when producing result, only need to garbage collect entries newer than the subterm
   there should be a single key allocator so that every value, even across partitions/worlds, has a unique key
     when partitioning based on hypothetical equality, copy all keys dependent on assumed value
       when re-combining worlds, new world only contributes keys newer than split
 assumptions in effect log mark when the world split
   new world not responsible for old effects, though may evaluate them under assumptions to see what they would provide
   when re-combining with old world, only effects after assumption are contributed
   after re-combining, assumptions are used to unify target keys and generate code to define new keys
 diagram:
   example of optimal assumption placement
   case D, where e's depend on D, c's do not depend on D at all, x's depend only on D's effect
 newer ---------------------------------- older
 ... e e e e (assume D = _) x x x D c c c c ...

 future small-step ideas
(data assumption
  (assume-eq (key0 key1))
  (assume-neq (key0 key1))
  (assume-value (key new-keys value)))
(data clg-entry
  (clg-data (kvs)))   plural, allowing SCCs (let-rec) to satisfy partition property
  (clg-obligation (key term env notes))
  (clg-assumption (assumed)))
  (clg-memory-effect ())
  (clg-stream-effect ())
  (clg-reset (marker))
  etc.
(data cont
  (ohc (cont oh))
  (halt ())
  (return-caller (cont env)))
  (return-context (cont env clg-replay)))

