reformulate terms to reduce redundancy and increase flexibility of reasoning
  symbols and conditionals will become derived concepts
  bits (zero and one) and pair-access will be introduced
    these can express pair-left/right and conditional branching

scrap the current promise framework
  replace with skolems plus a new propositional framework
  'skolem' renamed 'promise' in this system
    promise will then reflect an unknown atomic value
    propositions on promises will replace constraints of the current framework


; note: propositions really describe properties of full computational states,
;       not just thunks: catalog is implicit in the following

prop = thunk ==> thunk  ; step relation (==>* for reflexive transitive closure)
     | converges thunk atom  ; thunk terminates with atom as result
     | diverges thunk  ; thunk fails to terminate or produces an error
     | thunk === thunk  ; is this necessary? should it subsume atom equality?
     | atom == atom  ; judgmentally equal (commutative)
     | atom != atom  ; judgementally unequal (commutative)
     | type value-type atom  ; type constraint on atom

thunk = (term, env)

atom = unit | bit-0 | bit-1 | indirect UID | promise UID

value-type = Unit | Bit | Pair | Lam


derived props such as:
  thunk ===> atom, equivalent to something like:
  thunk ==> ((val-a atom), ENV)

contradictions produced by
  converges and diverges props for the same thunk
  == and != for the same pair of atoms
  multiple type constraints for the same atom

implications in the prop framework defined as meta-level lambdas
props are then data processed and produced by meta-level programs


new machinery
  catalogs now include propositions, describing hypotheses and/or inferences
    hypotheses are introduced at the meta-level by implications
  prop contradiction eliminates the current state from set of possibilities
  separate forcing/updating of delayed thunks from parent computation state
    you evaluate the thunk against the halt continuation
      but with the parent catalog
  lambda skolemization introduces new catalog frame: parent frame is read-only
  case splitting duplicates catalog frame for each new state case
  case splitting allows conjectures of the following forms:
    value-type cases
      the type of a promise is speculated before type-dependent evaluation
        (in the case of pair-access for bits/pairs or application for lams)
      each value-type conjecture starts a new hypothetical world/state
      as far as i can see, this will finally result in three situations:
        all cases converge, type splitting was unnecessary
        all cases diverge, type splitting did not save us
        only one case converges, type splitting made a difference
      no special term rebuilding necessary for this type of split
    bit-0 vs. bit-1
      this corresponds to exploring conditional branches
      each hypothetical world is rebuilt as a branch of a new conditional
    converge vs. diverge
      this corresponds to obligations for evaluating potentially-unused thunks
      if convergence cannot be proven for a thunk, a program is obligated
        to evaluate it even if its result is never used
      this obligation is reflected during term rebuilding
      proving divergence for a program thunk implies full program divergence
  observations about evolving converge/diverge hypotheses during evaluation
    corresponds to generating an obligation and promise
    as the thunk unwinds, the potentially-diverging part contracts
    contraction results in a new case split
      new diverging case obviates the original case split entirely
      new converging case contradicts original diverging case, eliminating it
      so, only the new child split/obligation remains
  for term rebuilding
    may need to keep a mapping of representative thunks for promises:
      promise UID -> (thunk OR skolem)
      the thunk introducing a promise is its representative

===============================================================================
= older ideas
===============================================================================

 eager and lazy CBV operational semantics
   CBV describes observable semantics while lazy/eager describes operational strategy
 eager CBV
   constructors with args need to save one-hole contexts
   all terms in focus are evaluated to completion with result values being catalogued
 lazy CBV
   constructors with args do not save any one-hole contexts
     they can punt on evaluating their args
     punted terms are paired with current environment and catalogued as eval obligations
   rewinding to evaluate an obligation
     pop catalog entries until desired key is found
     save popped entries (in reverse) with old context
     begin evaluating a new state with the remaining catalog
       (state clg-oblig-term (return-context (old-cont old-env reversed-entries)) clg-oblig-env)
     when returning to former context, re-push its entries onto the new catalog
   the catalog as described is actually a special case of a more general 'effect log'
     entry types can be added for memory allocation and writes
 given a catalog partitioning at any event boundary: members of older part do not depend on members of newer
   minimize duplication when splitting worlds around a hypothetical equality
     assumption boundary must be made before any key that would depend on it
       given key D being guessed
         assumption entry must be made before first E depending on D's value
         there may be entries between D and its assumption entry
           this would be because they depend on D's effects, but don't depend on D's value
       if made earlier than existing assumption, must split the splits
         this will happen with out-of-order case analysis on opaque values:
           first, case-analysis occurs on some D that happens to depend on C
           later, in one branch of (case D), retroactive case-analysis occurs on C
           the case-analysis on C must be pulled above that of D
             C's assumption entry must be made earlier than D's entry
             new split muts be made earlier than existing split, duplicating that split
             some waste produced (hopefully only temporarily) for the branch of D not analyzing C
       older keys definitely don't depend on assumption and make up the old region
       newer keys that happen to also not depend on assumption may be moved across it into the old region
         moving across also requires no effect dependencies
       old region ends up shared by both hypothetical worlds
   cleaning up after a transformation attempt on a subterm
     when producing result, only need to garbage collect entries newer than the subterm
   there should be a single key allocator so that every value, even across partitions/worlds, has a unique key
     when partitioning based on hypothetical equality, copy all keys dependent on assumed value
       when re-combining worlds, new world only contributes keys newer than split
 assumptions in effect log mark when the world split
   new world not responsible for old effects, though may evaluate them under assumptions to see what they would provide
   when re-combining with old world, only effects after assumption are contributed
   after re-combining, assumptions are used to unify target keys and generate code to define new keys
 diagram:
   example of optimal assumption placement
   case D, where e's depend on D, c's do not depend on D at all, x's depend only on D's effect
 newer ---------------------------------- older
 ... e e e e (assume D = _) x x x D c c c c ...

 future small-step ideas
(data assumption
  (assume-eq (key0 key1))
  (assume-neq (key0 key1))
  (assume-value (key new-keys value)))
(data clg-entry
  (clg-data (kvs)))   plural, allowing SCCs (let-rec) to satisfy partition property
  (clg-obligation (key term env notes))
  (clg-assumption (assumed)))
  (clg-memory-effect ())
  (clg-stream-effect ())
  (clg-reset (marker))
  etc.
(data cont
  (ohc (cont oh))
  (halt ())
  (return-caller (cont env)))
  (return-context (cont env clg-replay)))

