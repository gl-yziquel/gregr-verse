================================================================
= dependency graph database
================================================================

creation of new nodes
  definition of terms
  evaluation of terms
    evaluation traces track equalities between terms and their reductions and expansions

node data
  metadata/documentation
  original definition
  results of various analyses
    evaluation traces

lookup
  index/location (will this be useful outside of debugging? nodes may move)
  tag/uid
  uri (human names for uids of current/previous roots)
  direct term reference (traverse a dependency edge in the graph); references may need to be indirect to be efficient wrt node relocation (local env points to a particular uid instead of an index)
  ambiguous search by fuzzy name or by metadata
  definitional equality modulo renaming; probably something like a trie lookup
  ambiguous search by fuzzy definitional or analyzed property matching (ie, type-based search as in hoogle)
  homeomorphic embeddings; via tag bags?

gc: current roots, previously roots, never roots

a focus (see sub-db ideas below) corresponds to a collection of roots; would correspond to what the user (or some analysis) is or had been manipulating as a group; example: sub-graphs built and used by a supercompiler

producing completely standalone programs

producing programs that require dynamic linking
  may require manual identification of common dependencies to build specific dlls?
  or just consider these to depend on the entire graph db? then the program is simply like any other current root
  maybe the interesting idea here is to build sub-databases; although a sub-database is really just a particular collection of roots if it is not completely independent of a parent database; over the network, possible to send a copy of only part of the main db needed for a given program/focus
  sub-db defined as graph without any incoming dependency nodes from other dbs, but allowed to depend on other dbs
    interesting sub-dbs to track may be those that are maximal wrt a single focus; nodes shared by these 'focus dbs' could be analyzed to find natural dll boundaries, but dll may not be a useful concept
      this is starting to look like region analysis

================================================================
= supercompilation
================================================================

Figure out if it's worth attempting to supercompile "bottom up" when analyzing new definitions

supercompile reduces state as far as possible (until whistle #1, there's a whistle internal to reduce as well) before splitting; split will re-invoke supercompile on subterms
  this is memoized

evaluation rules: terminating portion for state normalization interleaved with full beta rule (deref indirection to apply function) and termination test (whistle #2)
  indirections: vars guaranteed to point to a 'value' in heap
  what about in a CBV language where laziness is explicit?

memo: name, free vars, alpha-normalized state; match via alpha-equivalence

split: stuck and whistled states

tag bags: tags only generated initially, one per program subterm; tag bag separates heap, focus, and stack tags
  tagging of reduced terms must somehow be based on existing tags
    set of tags should remain finite
  termination test, apply current state bag to all previous bags in history, terminate if the following is true: quasiLTEQbag b1 b2 <===> set(b1) == set(b2) AND |b1| <= |b2|
  rollback in reduce: store data (current state) in new history entries; "blamed" history entry state for whistle blowing is returned
  rollback in sc: similar idea reduce, but old invocation needs to retrieve newest state to analyze for generalization (chsc uses a throw/catch approach for this); unfulfilled promises and their transitive dependents have to be discarded upon rollback (they will never be fulfilled)
  generalization: terms with root marked by "blamed" tag (and their transitive dependencies according to chsc; does this really make sense?) are residualized at the unrolled state, and supercompiling continues there (residualized names don't occur in heap, and so end up being free vars, hence the generalization); if no tag is blamed, simply split and continue; not all blamed terms need to be residualized necessarily... chsc uses pruning heuristic where stack frame tags are residualized first; concept of computation-history "weight" may be useful (chsc uses this with growing literals built by primops)
  speculate heap bindings: attempt to reduce to values; check for non-termination due to new heap bindings being produced by speculative eval; rollback similar to sc throwing style

how to do lifting?
  example, lift if/else out of tuple construction?
  let b = odd unk in (if b then x else y;if b then y else x)

thoughts: (f x (g y z)) does not match something like (f x (g term z)) where x, y, z are free vars; is it better to supercompile the free var expr, then supercompile the more specific expr starting from the supercompiled more general free var version? how to figure out when to do this?

when analyzing a function definition, is there a nice way of determining which of its bound vars is "important" in terms of the supercompilation behavior? important to control flow?
  new answer?: yes, by looking at which values determine termination-relevant control flow
  example, if we define: f important peripheral = ... then finding a term of the form: f complex whatever; suggests that we might want to first supercompile something more general, such as: f simplified freevar; where simplified only retains important/central structure based on its own internal applications, converting peripheral terms to free vars
    this may allow us to automatically identify useful 'theorems'
    actually, this seems to be at least somewhat (maybe completely?) achieved by generalization already...

CBV sc is missing something: it seems to need a special way of generalizing contexts (and possibly heap bindings?), maybe treating them as if they are accumulators; related to this, CBV sc needs to do something when supercompiling constructors that CBN sc doesn't do, and how are dead/finished expressions treated here or in the context in general?
embedded-tuple lifting/tieback; maybe more accurate to call it 'vectorized' tieback, and extend it to work with any situation with linear applicative evaluation (unknown function call, constructor, etc.)
  possible to do this search when the original and final tuple constructor share the same syntax tag: just search each position for an embedded version of the original expr in that same position
  study this via a tuple-deconstructing let? or a new flavor of case?
  do constructors need a special rule or heuristic for finding improvement lemmas by parallel lifting? is that the right way to deal with this?
  can vectorization be undone easily in tuple deconstruction contexts? an alternative would be to use something like equality saturation for potential supercompilings of an expression.  When supercompiling an expression, for any given sub-expression, choose the best supercompiling wrt its context (choice between explicit tuple construction vs. vectorized recursive function; if the context is tuple deconstruction, the first is easier to work with)

================================================================
= correctness of transformations
================================================================

partial correctness (no observable difference in pure computations, ignoring effects)
  reductions correspond to small-step rules of evaluator (in any order)
  semantic equivalence of term rewrites/substitutions
    new term must fully reduce to the same value as the original term
operational correctness (no observable difference in effects, including termination)
  terms that always terminate or are always evaluated (strictness) regardless of reduction order may be reduced in any order
    those which do not fall into the above category must be reduced in the evaluator's canonical order
  impure terms additionally restrict order of reduction to avoid observable differences in side effects
    linearity: effects must not be duplicated, unless idempotent
    commutativity: order of effects must not be interchanged unless the effects are commutative
  operational equivalence of term rewrites/substitutions
    new term must fully reduce at least as quickly as the original term
    improvement condition of the HLSC paper

================================================================
= theorem proving
================================================================

transformation and theorem proving meta-level provided by reflection (somewhat inspired by milawa)
possible proof operations: total, strict, pure, defined?; rewrite, neutralize?, normalize, unfold, fold, abstract/generalize, erase, skip, split, finish, lemma?
liveness/productivity? effects?
memory safety policy for direct address space access/manipulation; ie, how to show gc, or anything built with exposure to direct memory allocation is safe
  memory effect notation: ME = VARIABLE | empty-memory | (alloc addr ME) | (dealloc? addr ME) | (write addr val ME) | (eval-effect term ME)
  what are addresses, really?
is mobile PCC more more effective for original/high-level code, or generated code?
can proofs for high-level language be straightforwardly transformed into proofs for low-level language during compilation?

================================================================
= implementation strategy
================================================================

to attempt to satisfy the de bruijn criterion, start simple:
  minimal pure functional language with basic data structures

given this simple foundation, attempt to build a more fully-featured high-level language
  example extensions:
    n-ary tuples
    mutable cells
    delimited continuations
    concurrency inspired by a process calculus
    linear resource analysis
    optional control over data representation
    bit-level values and operations
    os interaction
    foreign language interop

later, attempt to design a low-level language
  inspired by continuation-enhanced SSA language used in MLton?
    http://mlton.org/pipermail/mlton/2003-March/023325.html
    http://mlton.org/pipermail/mlton/2003-March/023326.html
    http://mlton.org/pipermail/mlton/2003-January/023054.html
  hopefully not just a compilation target
    use to describe runtime support (such as gc)
    use to describe a kernel
    these may be easiest to achieve through direct code generation via high-level language library

================================================================
= misc
================================================================

sources and sinks, with synchronization

effect commutativity: reads, writes, input/output streams; concurrency

(tuple/buffer-based?) cell/ref + take&put as both mutation and concurrency primitives; express compare-and-swap as a peep-hole optimization for specific machines
  OS/scheduling expressed in DSL for modelling (virtual) machines
    translation from model code to assembly
    execution of assembly: refer upward to host (virtual) machine, passing it the code to run?

tagged values are sealed

{type-repr a} -> a -> result

how to do supercompiler splitting/merging wrt records rather than cases?
