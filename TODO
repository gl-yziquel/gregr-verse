data representation for various stages of interpretation/compilation
  how will runtime code generation be supported?
  will modules store data needed to compile procedures it stores?
    maybe the procedure objects themselves should store this data repr
      erasable when unused; see value erasure somewhere below

system-level procedures for simple c-like translation? no gc; no auto-parallelism (w/ threads; optimal reordering is still fine)
  general procedures with module-like linking behavior with parallelism available in dependency order for sub-computations?

implicit union and intersection constructors
  are intersection constructors the same as record constructors? (and so unions are variants?)

type-inference algorithm
  do these dependently-typed terms have principal typings?  they don't need to; use CFA to deal with them
  can probably get away with it via syntactic embellishments or type annotations

dependent-typing functions can use (possibly run-time) partial evaluation / (JIT) abstract interpretation
  ex: sprintf :: (x::String) -> SPrintfType x

procedure calls as module linking:
  modules have two possible linking interfaces with corresponding procedure application analogies/protocols:
    name-based inputs (call w/ keyword args)
    position-based translation to input names (call w/ positional args, allowing possible skipping of positions)
  eval-within/proceed form asks for a given expression to be evaluated within a particular module
    if the module is complete, the result is the evaluation
    if the module is incomplete, the result is a "proceedure", though position-based calling is an orthogonal feature
      as the procedure is partially applied/linked, the results are also procedures
      once fully applied, the result is the evaluation
  procedure application translation:
    eval-within/proceed with the expression body and desired procedure module
    link args w/ procedure module via one of the above protocols
    value of the expression is produced if linking was complete; a partially-linked module is produced otherwise
  stack model:
    [...cont...[inputs]] - push args, then enter module computation
    [...cont...|[inputs][output space?]...locals...] - reserve space for results?, perform local computation
    [...cont...[former inputs?][outputs]] - continue with output results
    how to reconcile stack model of mutually-recursive linking?

incorporate fully-parallelizeable procedures:
  expose outer and inner evaluation parallelism (argument evaluation vs. body evaluation)
  synchronization points to control parallelism and allow side-effects
  fundamental syntax: apply-parallel and apply-serial
  evaluation under lambda: partially applied procedures; when is it appropriate? (strict vs. lazy argument returns)
  reconcile complexities in coordination of application site provisions with procedure body dependencies

value-level (semi?)unification:
  why should the type-level have all the fun?
  procedure-app linking coordinator can reach upwards from pattern argument into constructor computation to pull out
  the true dependencies asynchronously to avoid having to wait for an artificial synchronization due to constructor app

data repr:
  data types from universes as in Conor McBride's ornaments paper
  records/modules
  structs/tuples
  arrays - ephemeral dynamic arrays via linear types
  inductive/coinductive algebraic type layer
  pointed/cyclic?/lifted/mutable/linear/shared

records:
  form of subtyping based on row-types, as in Morrow
  free extension without overwriting duplicate fields -- lexically scoped labels
  extension becomes like environment extension
  record field access -- constant-time lookup via implicitly-passed label->index args
  environments as records: variable lookup as field lookup
  closure and arguments as records
  procedure types can include parameter names (symbols):
    f : (one : A) -> (two : B) -> ...
    f one two = ...
  partially/fully apply procedure to arguments by name or position, even with non-flat procedures:
    call procedure with a record supplying arguments
    f : (one : A) -> (two : B) -> (three : C) -> ...
      by name:
        (f :@ {two=...}) : (one : A) -> (three : C) -> ...
      or by position
        (f :@ {2=..., 3=...}) : (one : A) -> ...
    proc could be non-flat; closure would simply store pending name/position fields until applicable
    the type says eventually there are named params: two and three
      f : (one : A) -> (two : B) -> (three : C) -> ...
      f one = ... computation here ... eventually return (proc two three = ...)
      essentially:
        (f :@ {three=val}) ==> proc one two = f one two val
        though maybe not implemented this way; with enough info, could be smarter and possibly finish computations
        that only depend on three early, even within the nested proc
        per-argument partial-application-handling code wrapped with closures to do this?
    if names and positions are mixed, positions should take precedence

modules: (how much of this is compatible with the above records?)
  basic module form that does no special parsing or special syntactic expansion with macros defined within it
  a text/file-based module loader that is given a parser and can use expanders as they're defined
    there could also be an intermediate loader for pre-parsed concrete syntax trees
  syntactic expansion:
    involves a renaming of public symbols (which could be used by many modules) to private symbols unique to that module
    syntactic closures effectively close over private symbols to avoid symbol mis-captures
    syntactic closures expanding to external dependencies
      cause implicit linking of private names? public names still not available for use, preserving encapsulation
    macros considered dependent on final module env:
      they normally won't be available for export until all of a module's dependencies are satisfied
        this also means values that store macros (ie., building a list of macros) are also still unevaluated
      the final env can be forced complete early, producing a record with the macros satisfied; it doesn't mutate the module
        of course will also satisfy dependencies of those values that store macros, so they will also be present
  undefine as an interactive module-builder gimick:
    create a new module and load current definitions into it, minus the symbol being undefined, and continue there
      these definitions must not close over the previous module, to avoid capturing references to the old def
  define form creates a binding within the enclosing [sub]-module's top-level environment
    since these bindings are all placed in the same environment without extension, they are mutually-recursive
  a module stores distinct SCCs of mutually recursive procedures/thunks, noting remaining dependencies for each
    as definitions are evaluated, if they satisfy an existing SCC's dependencies, the SCC is relinked
      this linking is asymmetric:
        the dependent SCCs have their types updated
        types of the dependencies don't depend on how they are used (more generally, types only depend on what they use)
          though a separate use-based linking could be done to determine proc specializations
      such dependency satisfaction may cause previously distinct SCCs to be coalesced (they're shown to be mutually recursive)
    when an evaluation is actually taking place, its dependencies are calculated and extracted from the module environment
      unsatisfied dependencies in the current module env cause it to be rebuilt, incorporating any additional defs seen
        if the dependencies still aren't there, it indicates an 'undefined dependency' failure
  unresolve dependencies: should such computations be saved up until linking?
    cyclic dependencies without indirection (ie., thunks) are an error
    would be nice, and wrt side-effects, those should be recorded in the order in which their dependencies are resolved
      symmetric linking deterministically satisfies the side effects of one module before the other
        also need a way to order incoming dependency symbols to allow proper ordering of side effects as they are satisfied
      ex: should print out "hello, goodbye"
        print msg
        print "hello, "
        msg = "goodbye"
    then the side-effects should actually occur whenever the module is completed via linking, in the same order
      or, maybe discharge of side-effects can be a separate operation after the linking is complete
        or even anytime during partial linking for partially-completed side effects?
          in this case, maybe such discharge should even be repeatable?
  pre-procedures are open terms defined in a module where the module-level bindings are free
    procedures can be created on demand during module eval by providing pre-procs with the most up-to-date module context
    modules are immutable, so this context changes as linking occurs to reflect the newly-formed module
  finally, evaluating a module produces a record reflecting its top-level environment
    originally was thinking it was ok to project on any complete fields of a partially linked module
      now I think this might be a bad idea; it's also a simpler model to only consider this for completed modules
  asymmetric directed linking: one module's provisions are used to satisfy another's requirements
  symmetric mutually-recursive linking: combine requirements and provisions, but resolving mutually satisfied requirements
  mixins/traits become easy
    procedure app explainable via very simple directed linking? allows targeted parameter applicaton? (ie. skipping of params)
  dynamic 'opening' of modules: should specific symbols need to be specified?
  reflection/introspection/inspection of exposed fields; debugger/authority able to inspect private environment

environments:
  environments are records:
    normally immutable, except in the case of module evaluation, during the evaluation itself?
    the residual record produced by the module evaluation should be immutable
  get-environment produces the current procedure environment as an immutable record:
    since a module's environment changes as the module is evaluated, define a get-namespace for grabbing a ref to it
    what about get-environment within a procedure defined in a module currently being evaluated? that will still break
      but such procedures can be considered different each time a new definition is linked to its parent module
      this "evolving" model can be implemented with lazy updates, only performing them when a procedure is demanded
      or, update could be performed when a procedure's dependencies are linked/updated
    another possibility: consider premature get-environment uses to be referencing undefined vars (in a way it does, right?)
      or the module environment can be considered the last definition of a module, so premature refs to it are undefined?
      or maybe implicit internal env access is a bad idea, and envs should only be accessible given a complete module in hand

alternative module/environment model:
  rejecting this idea for now; convenience of the "evolving" model seems to outweigh its complexity
  instead of insisting on modules essentially bootstrapping themselves into existence, consider them constructed externally
  though this will prevent the definition of macros alongside with their uses (this is something like phase-separation)
  separate issue: would be nice to be able to update module definitions and have their dependents re-link themselves
    doing this causes fissure with running code in the old module context; after all, module update should not be mutation

let, let-rec, let-seq via record construction and use/open:
  use ({x=...} :+: {y=...})
  use {x=..., y=...}
  use {x=...}; use {y=...}
  or not?  instead build a sub-module whose top-level defs are mutually recursive

type classes:
  static/dynamic dispatch based on records: 'case' involves destructor/continuation-record fields labeled with type/data tags
  ranging over values too; not just types
  allow projections, something like: ProjClass Type symbol
  class constraints a form of type function? based on records
  data-polymorphic views: streams vs. lists, type class interaction with case

case analysis:
  not just a static syntax compilation into nested switches
  pattern matching as selecting the field of a record containing destructors/continuations
  able to dynamically build efficient record-based matching as dependencies are satisfied

implicits:
  based on dynamically scoped parameters/keys (also see racket's initial values and preserves idea)
  open/closed contexts for accepting outside implicits (maybe all dynamic params too, not just implicits?)
  allow reinterpretation of type classes; eg. sorting w/ implicit Ord definition override

algebraic data:
  reified types as values (these are NOT data constructor tags)
    if a type contains value-level details, some of those details may be erasable at run-time
    ex. if we know that a particular Maybe value is always a Just, we can represent the value without a tag at run-time
    this is not limited to tags: if we know that a particular value is always [1, 2, 3], that value may be erasable as well
    of course, erasure should not be observable
  tags (these are not types, see above)
  constructors
  destructors

type tower:
  allow values to be raised to type-level computations, and vice-versa; generalizes upwards (kinds, etc.)
  how to fit in propositions?
  intersections/unions/subtyping/recursive types - avoid universal/existential quantification
  principal typing property: is it available under these circumstances?
  R-Acyclic Semi-unification
  dynamic checking = type checking occurs in tandem with evaluation (one step ahead?)
  recursive types due to mutually-recursive module linking still needs consideration
  type families: consider procedure that dynamically produces new types of the same form
  sub-structural typing and delimited continuations: shift/reset, control/prompt, spawn, abort, etc.

syntax:
  context-sensitive syntactic values bound to variables (such as constructors); or should it be values, not variables?
  above explainable via syntactic records with context tags as field labels?
  partial macro expansion
  infix operator groups / relative ordering only within groups; ordering is nonsensical outside of groups
  macros may be used within the defining file-based module as they are defined
    need some way of noticing if a macro has been used before it is defined; this signifies some kind of error
    special form for use of a macro as a value?

debugging:
  time-travel via reversible computations
  explicitly non-deterministic side effects allowable in parallel computations (to allow things like debug printing)

low-level:
  thread-local keys
  local data alloc
  linear values to describe ephemeral data
  non-allocating computations
  reversible/conservative computations
  local vs. global mutability; sharing
  exact/inexact/arbitrary precision
  synch/asynch channels
  ffi: i/o ports (including sockets?); unboxed arithmetic/comparison/conversion
  how to relate to miasm?
  parameterized performance/cost estimation
  equality saturation

example typing (ignoring free-ness of int-to-string and length):

f x = case x of
  Int i -> f (int-to-string i)
  String s -> length s

f:(x -> r) x = (case x of
  Int i -> (f:(a -> b) (int-to-string:(Int -> String) i:Int):String):b
  String s -> (length:(String -> Int) s:String):Int
  ):r

f : f?.(x -> r) :.

0{
x:i
i:Int
f:(a -> b)
a:String
b:
r:b
}
||
1{
x:s
s:String
r:Int
}

simplify:

f : f?.(x -> r) :.
0{
x:Int
f:(String -> r)
r:
}
||
1{
x:String
r:Int
}

link with free var f (itself!)
for each instance of f(small), link with f(big)
for single f(small) instance in env 0, choose f(big)'s env paths where the following holds when f(small):(String -> r):
note: only the input should be used to filter... can't filter based on output without possible unsoundness
x <: String
env 1 has no instances of f, otherwise each instance would choose its own appropriate paths to link with

therefore only env 1 path, choose fresh names:
(x1 -> r1) :.
{
x1:String
r1:Int
}
link with env 0 and unify fs, (x1 -> r1) = (String -> r):
x1 = String => String = String
r1 = r
0{
x:Int
f:(String -> r)
x1:String
r1:Int
r:r1
}
simplify (f no longer free; can be gc'ed):
0{
x:Int
r:Int
}

result is:
f : (x -> r) :.
0{
x:Int
r:Int
} # aka Int -> Int
||
1{
x:String
r:Int
} # aka String -> Int

or:
f : (Int -> Int) & (String -> Int) :. empty

